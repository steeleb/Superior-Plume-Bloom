---
title: "Methods and Results Summary"
author: "ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
bibliography: references.bib
---

```{r, include = F}
knitr::opts_chunk$set(message = F, echo = F, error = F, warning = F)

packages <- c("tidyverse", "gt", "sf", "tmap")

package_loader <- function(x) {
    if (x %in% installed.packages()) {
      library(x, character.only = TRUE)
    } else {
      install.packages(x)
      library(x, character.only = TRUE)
    }
}

invisible(lapply(packages, package_loader))

model_version = "2024-04-26"

```

# Introduction

Remote sensing image classification is common in terrestrial applications (in
particular, land use and land cover), but has not been applied in aquatic
environments beyond general presence and absence of water and wetlands. The
primary exception to the use of image classification in aquatic environments is
assessing the presence of submerged aquatic vegetation ("SAV") (e.g.,
[@visser2018, @e.l.hestir2012]); however, these classification methods require
high resolution imagery with high spectral resolution often from land-based
high-resolution photography or unpersoned aerial vehicles ("UAVs").

In the Great Lakes (GL) region, much of the use of image classification is
completed using moderate resolution (e.g., Landsat, Sentinel, MODIS) satellite
images, focusing on mapping the distribution and types of wetlands throughout
the region ([@mohseni2023, @v.l.valenti2020]), as well as SAV distribution
throughout the system [@wolter2005]. Most of these analyses focus on a
relatively short temporal period (months to years), while a some span the entire
Landsat archive from the mid '80s through the recent past (e.g., [@amani2022]).

In the recent past, much attention has been paid to the apparent proliferation
of algal blooms in some of the clearest lakes, including Lake Superior (cite).
While detecting algal blooms from moderate-resolution satellite imagery is
difficult due to low temporal revisit frequency, time of day of acquisition,
pixel size, and spectral band metrics (cite), as well as the lack of observed,
spatially-explicit bloom observations to validate presence and absence, visually
detecting sediment plumes (which often precede algal blooms) is relatively easy
with just the red, green, and blue bands common on nearly all
moderate-resolution satellites.

In this analysis, we use the Landsat Collection 2 Surface Reflectance product
archive (missions 5 through 9, [@vermote2016, @masek2006]) and the Sentinel 2
Surface Reflectance product archive [@drusch2012], a novel crowd-sourced label
data set (eePlumB), and Google Earth Engine to create image classification
models to create a time series of rasters that enumerate sediment plumes across
the western arm of Lake Superior.

```{r fig-aoi, fig.dpi = 300, fig.align="center", fig.width=2, fig.height=1}
aoi <- read_sf("data/aoi/Superior_AOI_modeling.shp") %>%
  st_make_valid()

tmap_mode("view")
tm_basemap("CartoDB.Positron") +
  tm_shape(aoi) + tm_polygons(col = "#b97eea", alpha = 0.5) +
  tm_scale_bar()

```

:"Area of interest for this analysis in purple, consisting of a portion the
western extent of Lake Superior, the Apostle Islands, and Chequamegon Bay."

# Methods

## eePlumB

```{r echo=FALSE}
labels <- read_csv("data/labels/collated_label_data_v2023-07-20.csv") %>% 
  mutate(missDate = paste(mission, date, sep = "_"))
mod_labels <- labels %>% 
  filter(class %in% c("openWater", "cloud", "lightNearShoreSediment", "darkNearShoreSediment", "offShoreSediment"))
tvt_files <- list.files("data/labels/", pattern = "tvt", full.names = T)
for_tvt <- map(.x = tvt_files,
               .f = ~ read_rds(.x)) %>% 
  bind_rows()
```

Using the overarching architecture presented in the Global Rivers Obstruction
Database (GROD) [@yang2022] to engage volunteer observers, we crowdsourced class
labels for Landsat and Sentinel-2 images for the following classes: 'cloud',
'open water', 'light near shore sediment', 'dark near shore sediment', 'offshore
sediment', 'shoreline contamination', 'other', and 'algae bloom' using our Earth
Engine Plume and Bloom labeling interface ("eePlumB"). Dates for labeling were
limited to the months of April through November to avoid ice-on. During 2023,
`r length(unique(labels$vol_init))` volunteers labeled
`r length(unique(labels$missDate))` images resulting in a total of
`r nrow(labels)` labels within these classes. When the label set was limited to
open water, cloud, and sediment classes, `r nrow(mod_labels)` labels remained.

Though the eePlumB architecture was based on GROD, a major difference between
the two applications was GROD labeled the most recent cloud-free image, and in
eePlumB, we wanted to label multiple images across many satellites. This
required a bit more volunteer savvy in the eePlumB module. Because eePlumb was
not robustly designed for user experience, it was possible to export label data
that did not match the user-entered mission or date if the directions were not
followed precisely. In order to assure that the volunteer data were valid, we
re-pulled the data based on the volunteer-exported mission, date, and lat/lon
values. In order to provide verified data for band values across the open water
and sediment categories, numerous quality checks were performed:

1.  Any label that contained band names that were not part of the mission listed
    were automatically dismissed (e.g. if a Landsat 5 image listed B6, which is
    a thermal band and not exported as part of the Landsat 5 stack, the instance
    was removed)

2.  If the volunteer-pulled band value did not match the re-pull value for a
    majority of the labels in a given mission date, the mission date was
    dismissed

Additionally, pixel and scene level quality checks were peformed:

3.  Labels that were flagged by the Landsat or Sentinel QA bands and bits were
    dismissed (see Table XXX below for QA pixel masks)

4.  Labels belonged to a scene where there was scene-level contamination (cirrus
    clouds or atmospheric correction issues visible).

Any remaining extreme outlier in these classes (defined as \> 1.5\*IQR per band
per class) was retained if they were not dismissed in the above steps. Rendered
code (as .htmls) detailing these decisions is available in the Superior Plume
Bloom repository in the sub directory `modeling`, scripts 01-06. Generally
speaking, all cloud labels were kept as long as the pixel would not have been
masked as 'saturated' (QA_RADSAT == 0 for cloud label to be retained).

After the QA steps, `r nrow(for_tvt)` labels remained, Table 1 details the
remaining labels for building our classification models.

```{r echo=FALSE}
mission_class_summary <- for_tvt %>% 
  group_by(mission, class) %>% 
  summarize(n_labels = n()) %>% 
  pivot_wider(names_from = "class", values_from = "n_labels")  %>% 
  rename(`dark near shore sediment` = darkNearShoreSediment,
         `light near shore sediment` = lightNearShoreSediment,
         `off shore sediment` = offShoreSediment,
         `open water` = openWater)
mission_date_summary <- for_tvt %>% 
  group_by(mission) %>% 
  summarize(`number of unique dates` = length(unique(date)))
mission_summary <- full_join(mission_date_summary, mission_class_summary)
gt(mission_summary) %>%
  cols_align(., 'center', columns = 2:ncol(mission_summary)) %>% 
  tab_footnote('Table 1. Summary of eePlumB classes per mission after validity check (model version 2024-04-26).')
```

Three areas of interest (AOIs) were created in this workflow: the complete AOI, the
AOI without shoreline contamination, and the AOI with shoreline contamination.
The area of shoreline contamination was defined as any area within 60 meters of
a volunteer-identified pixel with shoreline contamination. We assumed that
shoreline contamination was consistent throughout the analysis and was not
specific to any particular satellite or time period (see
`modeling/07_Shoreline_Contamination.html`). Any classified pixel within the area
of shoreline contamination should be interpreted with caution, if used.

## Model development

We used the built-in gradient tree boost ("GTB") ee.Classifier() method within
Google Earth Engine to create classification models from the crowd-sourced label
data. Image-dates for each mission were split randomly 70%/30% training and
testing, except Landsat 9 data which required manual selection of the test set
such that all classes were present in both the training and testing data sets
due to the limited number of dates available for the Landsat 9 mission. We did
not tune the hyper parameters for the GTB model, electing to use default
settings and 10 trees (a conservative number of trees). We did not attempt to tune
hyper parameters. Since the GTB classifier does not require and explicit validation 
data set to build a model, and we did not complete any hypertuning, we determined 
it was acceptable to use only a train-test split. After a model was trained, 
test performance was obtained. If the model performance was acceptable (generally 
\>80% accuracy across all classes) and it was applied to the image stack for that mission.

GTB models for each mission (LS 5, 7, 8 ,9, Sentinel 2) were trained independently 
on the rescaled band data (0-1 values) from all available surface reflectance 
bands to classify the 5 categories: cloud, open water, light near shore sediment, 
dark near shore sediment, and offshore sediment. In addition to the 5-category 
classification, we trained a 3-class model. Preliminary data analysis revealed 
that there were classes in the 5-class group that were not statistically different 
from one another on an individual band basis (see the rendered .html 02-06 in the
modeling subdirectory of the repository).


## Image classification

### Image Pre-processing

Mosaic-ed images were made for each mission-date as mean band values where any
two path row or tiles overlapped. Sentinel-2 bands that had a pixel resolution
greater than 10m x 10m were resampled (downsampled) to 10m x 10m pixel sizes so
that the GTB model could be applied to the composite images more efficiently.

Pixel masking was slightly different for each mission. The following metadata
and pixel masks were applied to the stack to create the most conservative raster
output. All stacks were limited to the months of April-November to align with
the label data.

-   Landsat 5 and 7

    -   Scene-level filters: CLOUD_COVER \< 80, IMAGE_QUALITY \>= 7

    -   Pixel-level filters: SR_ATMOS_OPACITY \< 0.3 (only low opacity),
        QA_PIXEL masking of high-confidence cirrus clouds, snow/ice, cloud shadows, and clouds),
        QA_RADSAT = 0 (all bands must be unsaturated)

-   Landsat 8 and 9

    -   Scene-level filters: CLOUD_COVER \< 80, IMAGE_QUALITY \>= 7

    -   Pixel-level filters: SR_QA_AEROSOL bits 6-8 \< 3 (masking
        high-confidence aerosol), masking of high-confidence cirrus clouds, snow/ice, 
        cloud shadows, and clouds), QA_RADSAT = 0 (all bands must be unsaturated)

-   Sentinel 2A/2B

    -   Scene-level filters: CLOUDY_PIXEL_PERCENTAGE \< 80, GENERAL_QUALITY =
        'PASSED', GEOMETRIC_QUALITY = PASSED, SENSOR_QUALITY = 'PASSED',
        DEGRADED_MSI_DATA_PERCENTAGE < 10, SNOW_ICE_PERCENTAGE < 10

    -   Pixel-level filters: SCL != c(1, 2, 3, 9, 9, 10, 11), QA60 bit 10 = 0
        (masking opaque clouds), bit 11 = 0 (masking cirrus clouds)

        ![](images/SCL_classes.png)


### Model application and summaries

Each GTB model was applied to the corresponding satellite image stack, where a .tif raster at the
resolution the GTB was applied (10m for Sentinel-2 and 30m for Landsat) for each
classified mission-date image. The .tif rasters were labeled by pixel with the
following values: 0 = out of area/masked for saturated pixels; 1 = cloud; 2 =
open water; 3 = light, near shore sediment; 4 = offshore sediment; 5 = dark,
near shore sediment.


## Model evaluation metrics

Models were evaluated through error matrices, kappa statistics, and F1
statistics for each class.

-   error matrix - testing: given the test data, does the model assign the
    correct class? These are tibble-style summaries where the model-assigned
    class and label class are compared.
-   kappa statistic: an indicator of how much better or worse a model performs
    than by random chance. score is -1 to 1, where 0 is the same as random
    chance, positive values are better than random chance and negative are
    poorer than random chance
-   F1 score: the harmonic mean of precision and recall per class (beta = 1,
    hence F1 where precision and recall are evenly weighted). Scores of 0 means
    the model cannot predict the correct class, a score of 1 means the model
    perfectly predicts the correct class.

Models were evaluated as 5-class categories and 3-class categories where all
sediment categories were compiled into a single class. It is important to note
that while performance metrics on a per-pixel basis may be good, that doesn't 
ensure that the vector output is as accurate. We have no way of measuring edges of
classes or comparing area under curve (AUC), a common segementation accuracy method.

# Results

## Label dataset

```{r, include = F, echo = F, eval=FALSE}
label_table <- labels %>%
  group_by(mission, class) %>%
  tally() %>%
  pivot_wider(names_from = mission, values_from = n) %>%
  rename('Landsat 5 (all)' = 'LS5',
         'Landsat 7 (all)' = 'LS7',
         'Landsat 8 (all)' = 'LS8',
         'Landsat 9 (all)' = 'LS9',
         'Sentinel-2 (all)' = 'SEN2')

mod_label_table <- mod_labels %>%
  group_by(mission, class) %>%
  tally() %>%
  pivot_wider(names_from = mission, values_from = n) %>%
  rename('Landsat 5 (filtered)' = 'LS5',
         'Landsat 7 (filtered)' = 'LS7',
         'Landsat 8 (filtered)' = 'LS8',
         'Landsat 9 (filtered)' = 'LS9',
         'Sentinel-2 (filtered)' = 'SEN2')

filtered_label_table <- for_tvt %>%
  mutate(mission = if_else(grepl('Sentinel', mission), 'SENTINEL_2', mission)) %>% 
  group_by(mission, class) %>%
  tally() %>%
  pivot_wider(names_from = mission, values_from = n) %>%
  rename('Landsat 5 (qaqc)' = 'LANDSAT_5',
         'Landsat 7 (qaqc)' = 'LANDSAT_7',
         'Landsat 8 (qaqc)' = 'LANDSAT_8',
         'Landsat 9 (qaqc)' = 'LANDSAT_9',
         'Sentinel-2 (qaqc)' = 'SENTINEL_2')

label_table_join <- full_join(label_table, mod_label_table) %>% full_join(., filtered_label_table)
```

The collated crowdsourced label dataset consisted of `r nrow(labels)` labels
across all classes. There were `r nrow(mod_labels)` labels that were part of the
classes of interest (cloud, open water, sediment). After filtering for quality
from each subset of mission-specific labels, there were
`r nrow(for_tvt)` labels with complete band information. Table 1
presents a break down of the labels.

```{r, echo = F}
gt(label_table_join) %>%
  cols_align(., 'center', columns = c('Landsat 5 (all)':'Sentinel-2 (qaqc)')) %>%
  tab_spanner('all labels',
              c('Landsat 5 (all)':'Sentinel-2 (all)')) %>%
  tab_spanner('filtered for classes of interest',
              c('Landsat 5 (filtered)':'Sentinel-2 (filtered)')) %>%
  tab_spanner('quality assured labels for model development and testing',
              c('Landsat 5 (qaqc)':'Sentinel-2 (qaqc)')) %>%
  tab_style(cell_text(size = 'small'), cells_body()) %>%
  tab_style(cell_text(size = 'small'), cells_column_labels()) %>%
  tab_footnote('Table 2. Summary of eePlumB classes per mission (version 2023-07-20). All labels (left group) are the raw number of collated labels, the middle group contains only the classes of interest, the right group are labels that have been filtered as described for use in training and testing.')
```

```{r, include = F, echo = F, eval=F}
mission_date_summary <- mod_labels %>%
    mutate(mission = case_when(mission == 'LS5' ~ 'Landsat 5',
                               mission == 'LS7' ~ 'Landsat 7',
                               mission == 'LS8' ~ 'Landsat 8',
                               mission == 'LS9' ~ 'Landsat 9',
                               mission == 'SEN2' ~ 'Sentinel 2',
                               TRUE ~ NA_character_)) %>% 
  group_by(mission, date) %>%
  tally()
mission_date_summary_filtered <- for_tvt %>%
  mutate(mission = case_when(mission == 'LANDSAT_5' ~ 'Landsat 5',
                               mission == 'LANDSAT_7' ~ 'Landsat 7',
                               mission == 'LANDSAT_8' ~ 'Landsat 8',
                               mission == 'LANDSAT_9' ~ 'Landsat 9',
                               grepl('Sentinel', mission) ~ 'Sentinel 2',
                               TRUE ~ NA_character_)) %>% 
group_by(mission, date) %>%
  tally()
md_summ_table <- mission_date_summary %>%
  group_by(mission) %>%
  tally() %>%
  rename('unique mission-dates (filtered)' = 'n')
md_summ_table_filt <- mission_date_summary_filtered  %>%
  group_by(mission) %>%
  tally() %>%
  rename('unique mission-dates (qaqc)' = 'n')

summary_table_join <- full_join(md_summ_table, md_summ_table_filt)
```

Labels were present from `r nrow(mission_date_summary)` individual mission-date
combinations spanning the dates of `r min(mission_date_summary$date)` to
`r max(mission_date_summary$date)`. Labels in the train-test data set were present
from `r nrow(mission_date_summary_filtered)` mission-date combinations spanning
the dates `r min(mission_date_summary_filtered$date)` to
`r max(mission_date_summary_filtered$date)`. See Table 2 for a complete
breakdown of labels by mission-date combination.

```{r, echo = F, eval=FALSE}
gt(summary_table_join) %>%
  cols_align(., 'center', columns = c('unique mission-dates (filtered)', 'unique mission-dates (qaqc)')) %>%
  tab_style(cell_text(size = 'small'), cells_body()) %>%
  tab_style(cell_text(size = 'small'), cells_column_labels()) %>%
  tab_footnote('Table 3. Summary of eePlumB mission-dates present in the label dataset. When quality control applied, ten mission-dates were dropped from the dataset.')
```


## Model evaluation

Five-class model performance was much lower across discrete sediment categories (Table 4) than
three-class model performance was more accurate across cloud, open water and sediment 
classes (Table 5), resulting in a higher kappa statistic and more even F1 scores
across classes.

```{r, echo = F, eval=F}
# get a list of the performance metrics list
perf_metrics_list <- list.files("data/output/",
                                pattern = "_performance_stats.csv",
                                full.names = T)
# filter for only those for the desired model version
perf_metrics_list <- perf_metrics_list[grepl(model_version, perf_metrics_list)]

# subset for 5-class and 3-class
three_perf <- perf_metrics_list[grepl('3class', perf_metrics_list)]
five_perf <- perf_metrics_list[!perf_metrics_list %in% three_perf]
```

```{r}
five_perf_metrics <- map_dfr(five_perf, read_csv) %>%
  #drop accuracy, since it's not meaningful for uneven groups
  select(-GTB_accuracy) %>%
  mutate(across(c(cloud:GTB_kappa),
                ~ round(., 2))) %>%
  rename("cloud" = "cloud",
         "open water" = "openWater",
         "light near-shore sediment" = "lightNearShoreSediment",
         "off-shore sediment" = "offShoreSediment",
         "dark near-shore sediment" = "darkNearShoreSediment",
         "model kappa statistic" = "GTB_kappa")
gt(five_perf_metrics) %>%
  cols_align(., 'center', columns = c("cloud":"model kappa statistic")) %>%
  tab_spanner("classification F1 score",
              c("cloud":"dark near-shore sediment")) %>%
  tab_style(cell_text(size = 'small'), cells_body()) %>%
  tab_style(cell_text(size = 'small'), cells_column_labels()) %>%
  tab_footnote('Table 4. Summary of F1 scores per class and across-class kappa statisic for the 5-class GTB model.')
```

```{r}
three_perf_metrics <- map_dfr(three_perf, read_csv) %>%
  #drop accuracy, since it's not meaningful for uneven groups
  select(-GTB_accuracy) %>%
  mutate(across(c(cloud:GTB_kappa),
                ~ round(., 2))) %>%
  rename("cloud" = "cloud",
         "open water" = "openWater",
         "aggregated sediment" = "sediment",
         "model kappa statistic" = "GTB_kappa")
gt(three_perf_metrics) %>%
  cols_align(., 'center', columns = c("cloud":"model kappa statistic")) %>%
  tab_spanner("classification F1 score",
              c("cloud":"aggregated sediment")) %>%
  tab_style(cell_text(size = 'small'), cells_body()) %>%
  tab_style(cell_text(size = 'small'), cells_column_labels()) %>%
  tab_footnote('Table 5. Summary of F1 scores per class and across-class kappa statisic for the 3-class GTB model.')
```

In order to assess whether or not the 5-class models had any predictive power, 
we need to consider the confusion matrix. If the model misclassifications for 
sediment occur within other sediment categories, it's less troubling than if they
are misclassified as open water or clouds. The rows in the confusion matrix
indicate the modeled class, the columns indicate the labeled class. 

```{r echo = F, eval=F}
# load confusion files
confusion_files <- list.files("data/output",
                              pattern = c(model_version),
                              full.names = T)
confusion_files <- confusion_files[grepl('confusion', confusion_files)]
confusion_files <- confusion_files[!grepl("training", confusion_files)]

three_confusion <- confusion_files[grepl('3class', confusion_files)]

create_confusion_matrix_gt <- function(confusion_csv, 
                                       title = "Confusion Matrix",
                                       #subtitle = "Actual vs Predicted",
                                       tab_no, #table number
                                       no_class, #number of classes
                                       mission, #sat mission
                                       highlight_color = "lightblue",
                                       border_color = "grey",
                                       decimal_places = 0) {
  confusion_df <- read_csv(confusion_csv)
  
 # Selecting only the class and prediction columns
  confusion_data <- confusion_df %>%
    select(-mission) %>%
    rename(Actual = class)
  
  # Create the gt table
  gt_table <- confusion_data %>%
    gt() %>%
    tab_header(
      title = title,
      #subtitle = subtitle
    ) %>%
    fmt_number(
      columns = -Actual,
      decimals = decimal_places
    ) %>%
    tab_style(
      style = list(
        cell_fill(color = highlight_color),
        cell_text(weight = "bold")
      ),
      locations = cells_body(
        columns = Actual,
        rows = Actual == colnames(confusion_data)[-1]
      )
    ) %>%
    tab_style(
      style = cell_borders(
        sides = c("top", "bottom"),
        color = border_color,
        weight = px(2)
      ),
      locations = cells_body()
    ) %>%
    cols_align(
      align = "center",
      columns = everything()
    ) %>%
    cols_align(
      align = "left",
      columns = Actual
    ) %>%
    tab_options(
      table.width = pct(100),
      column_labels.font.weight = "bold"
    ) %>%
    cols_label(Actual = "Class") %>%
  tab_footnote(paste0('Table ', tab_no, '. Confusion matrix for the ', no_class, '-class GTB model for ', mission, '.'))
  
  return(gt_table)
}

```

```{r}
five_confusion <- confusion_files[!grepl('3class', confusion_files)]

five_confusion_mission <- map(five_confusion,
                              function(fn) {
                                str_split(fn, '_')[[1]][3]
                              })

pmap(list(confusion_csv = five_confusion,
          tab_no = as.character(6:10),
          no_class = list('five'),
          mission = five_confusion_mission),
     .f = create_confusion_matrix_gt)
)

```

As indicated by the confusion matrices, Landsat 5 and 7 have poor precision 
and/or recall performance in cloud and open water categories and does not seem to 
have accuracy discerning between sediment classes. Landsat 8 and 9 have poor accuracy in sediment classes, additionally Landsat 9 has no open water classes in 
the test set. 

```{r}
three_confusion <- confusion_files[grepl('3class', confusion_files)]

three_confusion_mission <- map(three_confusion,
                              function(fn) {
                                str_split(fn, '_')[[1]][4]
                              })

pmap(list(confusion_csv = three_confusion,
          tab_no = as.character(11:15),
          no_class = list('three'),
          mission = three_confusion_mission),
     .f = create_confusion_matrix_gt)

```

## Feature Importance



# Discussion

The GTB model was applied to all images in the Landsat and Sentinel 2 stacks,
irregardless of time of year and presence/absence of ice. Classified images
should only be used during ice-free periods, as no attempt was made to mask ice
or to classify ice. It is important to note that evaluation of the GTB model was
only done on the available by-pixel labels and that accuracy at classification
edges may not be precise.

In many cases, cirrus clouds are incorrectly classified as off-shore sediment.
Caution should be used when clouds characterize a large proportion of the AOI.

# Image Viewer Links

The following links are Google Earth Engine scripts that allow for manual
examination of the true color image, the eePlumB classification (version
2024-01-08), and a measure of atmospheric opacity (Landsat 5/7) or cirrus cloud
confidence level (Landsat 8 & 9). For Sentinel 2, the cirrus cloud indication is
a 0 (no cirrus detected) 1 (cirrus detected) value.

[Landsat
5](https://code.earthengine.google.com/3dc621a541fefa6db53e874646f93b13)

[Landsat
7](https://code.earthengine.google.com/83427128adac071d119edbd3a86f1127)

[Landsat
8](https://code.earthengine.google.com/f4aad47222c53d6cf7510ef3e3344119)

[Landsat
9](https://code.earthengine.google.com/f2983f2a2196a2c033afacd22471e398)

[Sentinel
2A](https://code.earthengine.google.com/c8ae30202ad9e2549f009babe736497c)

[Sentinel
2B](https://code.earthengine.google.com/31d9b913a8421091447f213ef6d1db6d)

Note, the Sentinel viewers may hang shortly before displaying the date list.
Landsat 5 and 7 opacity measure does not seem robust for detecting cirrus
clouds. More investigation is needed to determine cirrus cloud contamination in
those instances.

# References
