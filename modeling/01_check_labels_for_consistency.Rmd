---
title: "Check eePlumB labels for consistency"
author: "ROSSyndicate"
date: "2024-03-22"
output: html_document
---

```{r setup, echo = F}
# get all the functions from the src folder and load them
source_files <- list.files("src", full.names = T)
invisible(lapply(source_files, source))
# and the funcs for this analysis
analysis_funcs <- list.files("modeling/src", full.names = T)
invisible(lapply(analysis_funcs, source))
# list/load/download the packages needed for this script
packages <- c('rstatix',
              'tidyverse',
              'ggthemes'
              )
invisible(lapply(packages, package_loader))
```

# Purpose

This script looks through the eePlumB labels to check for inconsistencies between
the user-exported data and the re-pull of data. It also removes/flags any labels that 
may be contaminated with cirrus clouds.

## Load data

```{r}
collated_data <- read_csv("data/labels/collated_labels_with_additional_data_v2024-03-07.csv")
```

These data will need some wrangling. The column labels are not in an intuitive order,
and there is a mix of Sen2 and LS columns in each satellite group.

### Landsat 5/7

```{r}
LS5 <- collated_data %>% 
  filter(mission == "LS5") %>% 
  select(date, mission, class, vol_init, 
         # rename these for comparison's sake, these were re-named in the js code
         B1 = Blue, 
         B2 = Green, 
         B3 = Red, 
         B4 = B5, 
         B5 = B6, 
         B7, 
         # re-pull columns
         starts_with("SR"), starts_with("ST"), contains("QA"), 
         # drop ls8/9 temp band
         -c(ST_B10, cirrus, opaque_cloud),
         # drop sen/ls89 columns
         -c(SR_B6, SR_B8, SR_B8A, SR_B9, SR_B11, SR_B12, QA10, QA20, QA60),
         # drop ls89 qa column
         -contains("AERO")) %>% 
  mutate(across(c(B1:SR_B7),
                ~ round(., digits = 5)))

LS7 <- collated_data %>% 
  filter(mission == "LS7") %>% 
  select(date, mission, class, vol_init, 
         # rename these for comparison's sake, these were re-named in the js code
         B1 = Blue, 
         B2 = Green, 
         B3 = Red, 
         B4 = B5, 
         B5 = B6, 
         B7, 
         # re-pull columns
         starts_with("SR"), starts_with("ST"), contains("QA"), 
         # drop ls8/9 temp band
         -c(ST_B10, cirrus, opaque_cloud),
         # drop sen/ls89 columns
         -c(SR_B6, SR_B8, SR_B8A, SR_B9, SR_B11, SR_B12, QA10, QA20, QA60),
         # drop ls89 qa column
         -contains("AERO")) %>% 
  mutate(across(c(B1:SR_B7),
                ~ round(., digits = 5)))
```


### Landsat 8/9

```{r}
LS8 <- collated_data %>% 
  filter(mission == "LS8") %>% 
  select(date, mission, class, vol_init, 
         # rename these for comparison's sake, these were re-named in the js code
         B2 = Blue, 
         B3 = Green, 
         B4 = Red, 
         B5, 
         B6, 
         # re-pull columns
         starts_with("SR"), starts_with("ST"), contains("conf"), starts_with("QA"),
         # drop ls5/7 band
         -c(ST_B6, SR_ATMOS_OPACITY, SR_CLOUD_QA),
         # drop sen columns
         -c(SR_B8, SR_B8A, SR_B9, SR_B11, SR_B12, QA10, QA20, QA60)) %>% 
  mutate(across(c(B2:SR_B7),
                ~ round(., digits = 5)))

LS9 <- collated_data %>% 
  filter(mission == "LS9") %>% 
  select(date, mission, class, vol_init, 
         # rename these for comparison's sake, these were re-named in the js code
         B2 = Blue, 
         B3 = Green, 
         B4 = Red, 
         B5, 
         B6, 
         # re-pull columns
         starts_with("SR"), starts_with("ST"), contains("conf"), starts_with("QA"),
         # drop ls5/7 band
         -c(ST_B6, SR_ATMOS_OPACITY, SR_CLOUD_QA),
         # drop sen columns
         -c(SR_B8, SR_B8A, SR_B9, SR_B11, SR_B12, QA10, QA20, QA60)) %>% 
  mutate(across(c(B2:SR_B7),
                ~ round(., digits = 5)))


```


### Sentinel 2

```{r}
S2 <- collated_data %>% 
  filter(mission == "SEN2") %>% 
  select(date, mission, class, vol_init, 
         # rename these for comparison's sake, these were re-named in the js code
         B2 = Blue, 
         B3 = Green, 
         B4 = Red, 
         B5, B6, B8, B11, B12,
         # re-pull columns
         starts_with("SR"), 
         cirrus, opaque_cloud, starts_with("QA"), 
         starts_with("MSK"),
         # drop landsat bands
         -c(starts_with("ST"), contains("conf"), contains("AERO"),
            SR_ATMOS_OPACITY, SR_CLOUD_QA, QA_RADSAT, QA_PIXEL))

```


## Landsat 5 SR bands

Just look at the data to see 'how bad' the consistency is between the user-pulled
data and our pull:

```{r, echo=FALSE}
pmap(.l = list(user_band = c("B1", "B2", "B3", "B4", "B5", "B7"),
               ee_band = c("SR_B1", "SR_B2", "SR_B3", "SR_B4", "SR_B5", "SR_B7"),
               data = list(LS5),
               data_name = list("Landsat 5")),
     .f = make_band_comp_plot)
```

Look at the data that don't match, in this case, we'll just use B5 as a reference

```{r}
LS5_inconsistent <- LS5 %>% 
  filter(is.na(SR_B7) | 
           SR_B7 != B7)
```

Most of these are cloud labels, where the pixel is saturated, and then masked in 
the ee value - we can keep these as training data as long as the values match. Let's drop those from this subset and then look more.

```{r}
LS5_inconsistent <- LS5_inconsistent %>% 
  filter(!(class == "cloud" & is.na(SR_B7))) 
```

This leaves ~`r round(nrow(LS5_inconsistent)/nrow(LS5))*100, 1)` of the Landsat 
5 labels as inconsistent and shouldn't be included in the test/train set. 
Since we're not going to classify saturated pixels, we'll drop those 
too (these are instances where the pull we did will be NA but the user pull will 
have a value). Let's do a quick sanity check to make sure that we've dropped 
values that are inconsistent between pulls:

```{r}
LS5_filtered <- LS5 %>% 
  # filter data where SR_B7 has data and where the values match between the two
  # pulls and the SR values are less than or equal to 1
  filter((!is.na(SR_B7) & SR_B7 == B7 & SR_B7 <= 1) | 
           # or where the user-specified class is cloud and the pixel was saturated
           # providing no surface refelctance data
           (class == "cloud" & is.na(SR_B7))) %>% 
  filter(complete.cases(.))
pmap(.l = list(user_band = c("B1", "B2", "B3", "B4", "B5", "B7"),
               ee_band = c("SR_B1", "SR_B2", "SR_B3", "SR_B4", "SR_B5", "SR_B7"),
               data = list(LS5_filtered),
               data_name = list("Landsat 5")),
     .f = make_band_comp_plot)

```

And now let's look at the data by class.

```{r}
pmap(.l = list(data = list(LS5_filtered),
               data_name = list("Landsat_5"),
               band = c("SR_B1", "SR_B2", "SR_B3", "SR_B4", "SR_B5", "SR_B7")),
     .f = make_class_comp_plot)
```

We aren't actually modeling "other" or "shoreline contamination", so let's drop
those.

```{r}
LS5_class_analysis <- LS5_filtered %>% 
  filter(!(class %in% c("other", "shorelineContamination")))
pmap(.l = list(data = list(LS5_class_analysis),
               data_name = list("Landsat_5"),
               band = c("SR_B1", "SR_B2", "SR_B3", "SR_B4", "SR_B5", "SR_B7")),
     .f = make_class_comp_plot)


```

### Testing for inter-class differences

In order to test for differences via ANOVA we need to know if the data are normally 
distributed. So shapiro-wilkes first to test for normality!   

```{r}
shapiro.test(LS5_filtered$SR_B1)
shapiro.test(LS5_filtered$SR_B2)
shapiro.test(LS5_filtered$SR_B3)
shapiro.test(LS5_filtered$SR_B4)
shapiro.test(LS5_filtered$SR_B5)
shapiro.test(LS5_filtered$SR_B7)

SR_B1_median <- LS5_filtered %>% 
  group_by(class) %>% 
  summarize(median = median(SR_B1))
```

Null rejected. Treat all as non-normal, so Kruskal rank sum.

```{r}
SR_B1 <- kruskal.test(LS5_filtered$SR_B1 ~ LS5_filtered$class)
```

Null rejected. This makes sense, the cloud groups are different. Let's drop those
and test again.

```{r}
LS5_no_cloud <- LS5_filtered %>% 
  filter(class != "cloud")

SR_B1_nocloud <- kruskal.test(LS5_no_cloud$SR_B1 ~ LS5_no_cloud$class)
SR_B2_nocloud <- kruskal.test(LS5_no_cloud$SR_B2 ~ LS5_no_cloud$class)
SR_B3_nocloud <- kruskal.test(LS5_no_cloud$SR_B3 ~ LS5_no_cloud$class)
SR_B4_nocloud <- kruskal.test(LS5_no_cloud$SR_B4 ~ LS5_no_cloud$class)
SR_B5_nocloud <- kruskal.test(LS5_no_cloud$SR_B5 ~ LS5_no_cloud$class)
SR_B7_nocloud <- kruskal.test(LS5_no_cloud$SR_B7 ~ LS5_no_cloud$class)

```

Null rejected. Now for some pairwise comparisons using Wilcoxon rank test.

```{r}
# look to see which are closest
SR_B1_median %>% arrange(median)
# open water vs darkNS
df <- LS5_no_cloud %>%
  filter(class %in% c("openWater", "darkNearShoreSediment")) 
wilcox.test(df$SR_B1 ~ df$class)
# open water vs darkNS
df <- LS5_no_cloud %>%
  filter(class %in% c("offShoreSediment", "darkNearShoreSediment")) 
wilcox.test(df$SR_B1 ~ df$class)
# offshore vs lightNearshore
df <- LS5_no_cloud %>%
  filter(class %in% c("offShoreSediment", "lightNearShoreSediment")) 
wilcox.test(df$SR_B1 ~ df$class)
adj_pval = 1-((1-pval)^5)
# darkNS vs lightNearshore
df <- LS5_no_cloud %>%
  filter(class %in% c("darkNearShoreSediment", "lightNearShoreSediment")) 
wilcox.test(df$SR_B1 ~ df$class)

# adjust the pvalues for multiple comparisons
p.adjust(c(pval1, pval3, ...))
p.adjust(c(0.0000000001, 0.000000003, 0.05), method = "bonferroni")
```


## Landast 7 SR bands

Just look at the data to see 'how bad' the consistency is between the user-pulled
data and our pull:

```{r}
pmap(.l = list(user_band = c("B1", "B2", "B3", "B4", "B5", "B7"),
               ee_band = c("SR_B1", "SR_B2", "SR_B3", "SR_B4", "SR_B5", "SR_B7"),
               data = list(LS7),
               data_name = list("Landsat 7")),
     .f = make_band_comp_plot)
```

Look at the data that don't match, in this case, we'll just use B1 as a reference

```{r}
LS7_inconsistent <- LS7 %>% 
  filter(is.na(SR_B1) | 
           SR_B1 != B1)
```

```{r}
LS7_inconsistent <- LS7_inconsistent %>% 
  filter(!(class == "cloud" & is.na(SR_B1)))

```

The number of LS7 
labels being dropped is pretty high - so we might want to dig a bit more into 
that. 

Looking at the data frame for LS7 inconsistencies, it's clear there is at least
one image-date that should be excluded from this dataset, as most of the labels
do not match - 2020-07-18 by BJM. No other date/labelers seem to be consistently
mis-matched - this date accounts for more than half of the inconsistencies:

```{r}
LS7_inconsistent %>% 
  filter(date == "2020-07-18", vol_init == "BJM") %>% 
  nrow(.)
```


```{r}
LS7_filtered <- LS7 %>% 
  # filter data where SR_B1 has data and where the values match between the two
  # pulls and the SR values are less than or equal to 1
  filter((!is.na(SR_B1) & SR_B1 == B1 & SR_B1 <= 1) | 
           # or where the user-specified class is cloud and the pixel was saturated
           # providing no surface refelctance data
           (class == "cloud" & is.na(SR_B1))) %>% 
  # filter out the problematic date/init
  filter(!(date == "2020-07-18" & vol_init == "BJM")) 
pmap(.l = list(user_band = c("B1", "B2", "B3", "B4", "B5", "B7"),
               ee_band = c("SR_B1", "SR_B2", "SR_B3", "SR_B4", "SR_B5", "SR_B7"),
               data = list(LS7_filtered),
               data_name = list("Landsat 7")),
     .f = make_band_comp_plot)
```

```{r}

pmap(.l = list(data = list(LS7_filtered),
               data_name = list("Landsat_7"),
               band = c("SR_B1", "SR_B2", "SR_B3", "SR_B4", "SR_B5", "SR_B7")),
     .f = make_class_comp_plot)


```


```{r}
LS7_class_analysis <- LS7_filtered %>% 
  filter(!(class %in% c("other", "shorelineContamination")))
pmap(.l = list(data = list(LS7_class_analysis),
               data_name = list("Landsat_7"),
               band = c("SR_B1", "SR_B2", "SR_B3", "SR_B4", "SR_B5", "SR_B7")),
     .f = make_class_comp_plot)
```

